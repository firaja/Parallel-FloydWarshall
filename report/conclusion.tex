\section*{Conclusion}

In this work we described three approaches to parallelize the FW algorithm with three different architectures:
distributed with MPI, shared-memory multiprocessing with OpenMP and GPGPU with CUDA.

The fastest "pure" implementation is \emph{CUDA FW} thanks to the high computation capability and high memory bandwidth, 
but it requires more expensive hardware. 

\emph{MPI FW} (with one thread per node)  is still faster than the \emph{serial}, but the implementation
is more complex, the cost of the cluster (hosting and maintenance) makes the solution non convenient,
the efficiency depends on the network bandwidth and the speedup is not that high.

\emph{OpenMP FW} is almost 95 times faster than \emph{serial FW} but 13 times slower than \emph{CUDA FW}. The absence of
overhead due to communication, fast memory access, easy development process and relatively low costs make this solution the
most affordable, maintenable and cost-efficient one.

Talking about hybrid solutions, \emph{MPI + CUDA} is obviously faster than \emph{MPI + OpenMP} as long as the matrix is not small, 
but the benefits may not justify the total cost of the infrastructure: the monthly cost for hosting a server with a GPU is at least
four times the cost of one without a GPU. This solution is suggested to those systems that really need the lowest response time possible
(\emph{e.g.} real-time systems or systems that cannot rely on a caching mechanism in front of them).