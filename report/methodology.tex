\section{Methodology}
It is easy to notice that that the nested $i$ and $j$ for-loops in Alg. \ref*{alg:fw1} are totally independent and therefore parallelizable. \\
In this section we describe three strategies to overcome the iniefficiency due to a mono-thread computation.


\subsection{Distributed with MPI}

....
The main strategy is based on scattering the whole matrix among all the process, so that
each process can read a portion of the matrix of size $\frac{n^2}{p}$; then a \emph{process of competence}
is chosen: as $k$ is in common (but never transmitted) to all the processes, there's always a cell in the $k$th
row representing one of the two intermediate vertices for any process and there's always one process to which this row was assigned.
The value $k$ is always "synchronized" between process because each for-loop involving $k$ starts with a collective communication
which implies a synchronization point among processes.

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/mpi-scatter}
\captionsetup{justification=centering}                                                                                                                                   
\caption{View of the data each thread can reach}                                                                                                                                            
\label{fig:threads}                                                                                                                                                           
\end{figure}
Everytime $k$ changes, the $k$th row is broadcasted to the other processes by the process that owns the row;
a total of $k$ \texttt{MPI\_Bcast} operations are required. \par
Once the $k$th row has been received, each process acts like the original FW in the $i$ and $j$ for-loops; obviously
they each write values in their own local matrix. \par

At the end of the $k$ for-loop, all the local matrices are gathered to the root process. \par
We can list all the communication required:
\begin{itemize}
\item{1 \texttt{MPI\_Bcast} for communicating the value of $n$}
\item{1 \texttt{MPI\_Scatter} for the assignment of the local sub-matrix}
\item{$k$ \texttt{MPI\_Bcast} for communicating the $k$th row}
\item{1 \texttt{MPI\_Gather} for the collection of the local sub-matrix}
\end{itemize}

\textbf{Table \ref*{tab:comm}} approximates how many bytes are involved in the communications, assuming that
the implementation uses 4 bytes to store one \texttt{int} and omitting the overhead of the communication protocol.
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\rowcolor[HTML]{F56B00} 
{\color[HTML]{FFFFFF} \textbf{Count}} & {\color[HTML]{FFFFFF} \textbf{Type}} & {\color[HTML]{FFFFFF} \textbf{Size}} \\ \hline
1                                     &  \texttt{MPI\_Bcast}                 &  $4(p-1)$ bytes                      \\ \hline
1                                     &  \texttt{MPI\_Scatter}               &  $4\frac{n^2(p-1)}{p}$ bytes         \\ \hline
$k = n$                               &  \texttt{MPI\_Bcast}                 &  $4n^2(p-1)$ bytes                    \\ \hline
1                                     &  \texttt{MPI\_Gather}                &  $4\frac{n^2(p-1)}{p}$ bytes         \\ \hline
\end{tabular}
\caption{Execution time of the \emph{MPI} FW}                                                                                                                                            
\label{tab:comm}
\end{table}
\par

Total communications can be expressed with the following formula, that can help to calculate the bandwidth of the network
for this implementation:
\[W_{comm} = 4(p-1)(1 + n^2(1 + \frac{2}{p})) \text{ bytes}\]
This formula is important because the time spent on communications is time taken from the calculation and a network with an inadequate
bandwith is the main source of bottlenecks. For example having 8 processes consuming a $125000 \times 125000$ matrix implies a total of
$\textasciitilde 5.46$GB transferred  over the entire network; this means that each processor would theorically spend $5468$ms (43 seconds in total) on communication over a 1Gbps network.

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/mpi-time}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Calculation and MPI instantiation + communication time of \emph{MPI} FW consuming a $5040\times5040$ matrix over a 1Gbps network depending on the number of processes}
\label{fig:mpi-time}                                                                                                                                                           
\end{figure}

\textbf{Figure \ref*{fig:mpi-time}} shows the amount of time taken to initialize the MPI cluster and communicate during the execution of the \emph{MPI} FW. In this case a dense 
$5040\times5040$ matrix is used for input and the MPI cluster was located in a star network with 1Gbps bandwith.

The overhead has a quadratic or linear growth with respect to the number of vertices and or the number of processes respectively; 
so with a medium-small matrix the time used in communication can exceed the 10\% of the total computational time. 
But by increasing the size of the matrix to $12600\times12600$ the time used in communication drops to 4\%, with a total of 50 seconds. Thus the implementation fits well for huge matrixes, rather 
than medium ones.

For this reason the speedup is considerable but far from the ideal for a $5040\times5040$ matrix. \textbf{Figure \ref*{fig:mpi-speedup}} shows the trend of the speedup in relation to the 
sequential version.

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/mpi-speedup}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Speedup of \emph{MPI} FW}                                                                                                                                            
\label{fig:mpi-speedup}                                                                                                                                                           
\end{figure}

With a consequent decrease in efficiency as shown in \textbf{Figure \ref*{fig:mpi-efficiency}}
\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/mpi-efficiency}
\captionsetup{justification=centering}                                                                                                                                   
\caption{Efficiency of \emph{MPI} FW consuming a medium matrix}                                                                                                                                            
\label{fig:mpi-efficiency}                                                                                                                                                           
\end{figure}























































\subsection{Multithreading with OpenMP}
OpenMP (Open Multi-Processing) is an application programming
interface (API) for parallel programming intended to work on shared-
memory architectures. More specifically, it is a set of compiler
directives, library routines and environmental variables, which in-
fluence run-time behavior. OpenMP enables parallel programming in
various languages, such as C, C++ and FORTRAN and runs on most
operating systems. \par
The OpenMP API uses the fork-join model of parallel execution.
Multiple threads perform tasks defined implicitly or explicitly by
OpenMP directives. All OpenMP applications begin as a single thread
of execution, called the initial thread. The initial thread executes
sequentially until it encounters a parallel construct. At that point,
this thread creates a group of itself and zero or more additional
threads and becomes the master thread of the new group. Each thread
executes the commands included in the parallel region, and their
execution may be differentiated, according to additional directives
provided by the programmer. At the end of the parallel region, all
threads are synchronized. \par
The runtime environment is responsible for effectively scheduling
threads. Each thread, receives a unique id, which differentiates it
during execution. Scheduling is performed according to memory
usage, machine load and other factors and may be adjusted by altering
environmental variables. In terms of memory usage, most variables in
OpenMP code are visible to all threads by default. However, OpenMP
provides a variety of options for data management, such as a thread-
private memory and private variables, as well as multiple ways of
passing values between sequential and parallel regions. Additionally,
recent OpenMP implementations introduced the concept of tasks,
as a solution for parallelizing applications that produce dynamic
workloads. Thus, OpenMP is enriched with a flexible model for
irregular parallelism, providing parallel while loops and recursive
data structures. \par
The main advantage on using OpenMP is the ease of developing parallelisms
with simple constructs that do not differ too much from the original implementation.

The following snippet shows the implementation used in this work: the matrix containing the
distances between vertices is shared among all the threads, while the 3 nested for-loops are executed
by each thread indipendentely.



\begin{lstlisting}[style=CStyle]
int i, j, k;
#pragma omp parallel num_threads(t) shared(M) private(k) {
	for(k = 0; k < n; k++) {
		#pragma omp for private(i,j) schedule(dynamic)			
		for(i = 0; i < n; i++) {
			for(j = 0; j < n; j++) {
				if(M[i][j] > M[i][k] + M[k][j] || M[i][j] == 0) {
					M[i][j] = M[i][k] + M[k][j];
				}
			}
		}
	}
}
\end{lstlisting}

A dynamic scheduler works better than a static one in this case because the
density of the edges may vary and the compiler cannot foresee the content of the matrix. \\
Also note that the solution does not implement a \texttt{collapse} directive like this:
\begin{lstlisting}[style=CStyle]
...
for(k = 0; k < n; k++) {
	#pragma omp for ... collapse(2)			
	for(i = 0; i < n; i++) {
		for(j = 0; j < n; j++) {
			...
		}
	}
}
\end{lstlisting}
that's because when we collapse multiple loops, OpenMP turns them into a single loop: there is one
index that is constructed from \texttt{i} and \texttt{j} using division and modulo operations. This can
have huge impacts on the performance because of this overhead, especially if the matrix is really wide.

\textbf{Figure \ref*{fig:threads}} shows how 2 threads interact inside the matrix: the red and orange zones highlight the cells where
Thread 1 and Thread 2 can write respectively; the blue and turquise cells represent the intermediate vertices that are compared
with the vertice under analysis for Thread 1 and Thread 2 respectively.

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/openmp-threads}
\captionsetup{justification=centering,margin=2cm}                                                                                                                                   
\caption{View of the data each thread can reach}                                                                                                                                            
\label{fig:threads}                                                                                                                                                           
\end{figure}

It may seems that this implementation is affected by data races, because there's no lock when writing in \texttt{M} 
and one of the intermediate cells could be the same under analysis on another thread; for example, Thread 1 could write
in cell $M_{i,j}$ before or after that Thread 2 calculates $M_{x,j} > M_{x,i} + M_{i,j}$, making the result unpredictable.

We prove that no race condition may appear in this case. Let's assume we have 2 threads, namely $T^1$ and $T^2$ and they are
analyzing cell $(i,j)$ and $(x,y)$, with $x \neq i$. We denote the previous statement simply with $T^{1}_{i,j}$ and $T^{2}_{x,y}$.
Having in common $k$, the two threads are now calculating the following system

\begin{flalign}\label{eq:sys1}
 &&  \left\{\begin{matrix}
T^{1}_{i,j} & > & T^{1}_{i,k} & + & T^{1}_{k,j} \\
\\ 
T^{2}_{x,y} & > & T^{2}_{x,k} & + & T^{2}_{k,y}
\end{matrix}\right. &&
\end{flalign}

The two inequalities are taken from the condition of the \emph{if} condition in \textbf{Algorithm \ref*{alg:fw1}}. \\ 
If both are true, then $T^{1}$ and $T^{2}$ are allowed to write in $M$. In order to have a data race condition, the following
must be true
\[(k = i \wedge y = j) \vee (x = i \wedge k=j)\]
but since $x \neq i$, only the following must be verified

\begin{flalign}\label{eq:cond1}
 &&  k = i \wedge y = j &&
\end{flalign}
By applying (\ref*{eq:cond1}) to (\ref*{eq:sys1}) we have

\begin{flalign}\label{eq:sys2}
 &&  T^{1}_{i,j} > T^{1}_{k,k} + T^{1}_{i,j} &&
\end{flalign}
but (\ref*{eq:sys2}) is clearly false, because $M$ is a hollow matrix \emph{i.e.} the  diagonal elements are all equal to $0$, leaving the 
following inequality:
\begin{flalign}\label{eq:sys3}
 &&  T^{1}_{i,j} > T^{1}_{i,j} &&
\end{flalign}
Clearly no number can be greater than itself and this means that only $T^2$ may write in $M$ at this point. \\
No race condition can appear if $k$ is the same among the threads and threrfore there's no need to verify atomicity of the write operation;
the lack of OpenMP directive like \texttt{atomic} or \texttt{critical} plays in favor of performance. \par

The speedup of the solution is sligthy worse than the ideal speedup (see \textbf{Figure \ref*{fig:omp-speedup}})

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/openmp-speedup}
\captionsetup{justification=centering,margin=2cm}                                                                                                                                   
\caption{Speedup of \emph{OpenMP} FW on a octacore CPU}                                                                                                                                            
\label{fig:omp-speedup}                                                                                                                                                           
\end{figure}
When scaling from 7 to 8 threads, we notice a slight deviation from the previous (almost) linear trend. That's because the measurement
is taken from a 8-core/8-thread CPU, namely Intel Core i7-9700K, and because no other cores were free to manage the OS and its subprocesses, the scheduler
divided this task among all the threads. So we have approximated the speedup without counting the fluctuations due to the management of the OS. \par

The efficiency, which stays always above $90\%$, is shown in \textbf{Figure \ref*{fig:omp-efficiency}} alongside with its theoretical counterpart.

\begin{figure}[h!]
\centering                                                                        
\includegraphics[width=3.5in]{diagrams/openmp-efficiency}
\captionsetup{justification=centering,margin=2cm}                                                                                                                                   
\caption{Efficiency of \emph{OpenMP} FW on a octacore CPU}                                                                                                                                            
\label{fig:omp-efficiency}                                                                                                                                                           
\end{figure}
An overview of the timings collected can be found in \textbf{Table \ref*{tab:omp-time}}.













































































\subsection{GPGPU with CUDA}
...

